{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwaLK4kD1Uar"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiUyHP4T2g5F"
   },
   "source": [
    "# Install the dependicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LL4rxT6_W7h"
   },
   "outputs": [],
   "source": [
    "!pip install llama-index\n",
    "!pip install langchain\n",
    "!pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbuYetOy25eM"
   },
   "source": [
    "# Define the functions\n",
    "The following code defines the functions we need to construct the index and query it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UelAqQgk_yIt"
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, VectorStoreIndex, LLMPredictor, PromptHelper, ServiceContext, StorageContext, load_index_from_storage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "# from langchain import OpenAI\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def construct_index(directory_path):\n",
    "    # set maximum input size\n",
    "    max_input_size = 4096\n",
    "    # set number of output tokens\n",
    "    num_outputs = 2000\n",
    "    # set maximum chunk overlap\n",
    "    max_chunk_overlap = 0.2\n",
    "    # set chunk size limit\n",
    "    chunk_size_limit = 600\n",
    "\n",
    "    # define prompt helper\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    # define LLM\n",
    "    # llm_predictor = LLMPredictor(llm=OpenAI(temperature=0.5, model_name=\"gpt-3\", max_tokens=num_outputs))\n",
    "    llm_predictor = LLMPredictor(llm=ChatOpenAI(openai_api_key=openai.api_key, temperature=0.5, model_name=\"gpt-4\", max_tokens=num_outputs))\n",
    "\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "    index.storage_context.persist()\n",
    "\n",
    "    return index\n",
    "\n",
    "def ask_ai():\n",
    "    # index = VectorStoreIndex.load_from_disk('index.json')\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    while True:\n",
    "        query_engine = index.as_query_engine()\n",
    "        query = input(\"Hello! How may I help today? \")\n",
    "        response = query_engine.query(query)\n",
    "        # response = index.query(query)\n",
    "        display(Markdown(f\"Response: <b>{response.response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz1jp33jGumu"
   },
   "source": [
    "# Set OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoJHE4fsAT3w"
   },
   "outputs": [],
   "source": [
    "# os.environ[\"OPENAI_API_KEY\"] = input(\"Please enter your OpenAI API key: \")\n",
    "import openai\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVrddlAL4I_v"
   },
   "source": [
    "# Construct an index\n",
    "Now we are ready to construct the index. This will take every file in the folder 'data', split it into chunks, and embed it with OpenAI's embeddings API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCYTE2EqBB7O"
   },
   "outputs": [],
   "source": [
    "construct_index(\"./data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipJ_gYxN5cWh"
   },
   "source": [
    "# Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_uwsPGEIGsb"
   },
   "outputs": [],
   "source": [
    "ask_ai()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
