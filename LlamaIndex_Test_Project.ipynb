{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZwaLK4kD1Uar"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this project, we are leveraging the LlamaIndex technology to build a simple chatbot. The chatbot utilizes OpenAI's GPT-4 model to answer queries based on the indexed documents. The following steps will guide you through the process of setting up the necessary environment, constructing an index with the documents in the 'data' directory, and initiating a chat interface where you can ask questions and receive answers from the AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XiUyHP4T2g5F"
   },
   "source": [
    "# Install the dependicies\n",
    "\n",
    "1. llama-index: A package that facilitates the creation of an index with LlamaIndex technology.\n",
    "2. langchain: A package that provides chat models to be used with LlamaIndex.\n",
    "3. sentence_transformers: A package necessary for embedding sentences with transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6LL4rxT6_W7h"
   },
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install langchain\n",
    "%pip install sentence_transformers\n",
    "%pip install python-dotenv\n",
    "%pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbuYetOy25eM"
   },
   "source": [
    "# Define the functions\n",
    "The following code defines the functions we need to construct the index and query it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UelAqQgk_yIt"
   },
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTListIndex, readers, VectorStoreIndex, LLMPredictor, PromptHelper, ServiceContext, StorageContext, load_index_from_storage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Define the functions\n",
    "def construct_index(directory_path):\n",
    "    # set maximum input size\n",
    "    max_input_size = 4096\n",
    "    # set number of output tokens\n",
    "    num_outputs = 2000\n",
    "    # set maximum chunk overlap\n",
    "    max_chunk_overlap = 0.2\n",
    "    # set chunk size limit\n",
    "    chunk_size_limit = 600\n",
    "\n",
    "    # define prompt helper\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    # define LLM\n",
    "    llm_predictor = LLMPredictor(llm=ChatOpenAI(openai_api_key=openai.api_key, temperature=0.5, model_name=\"gpt-4\", max_tokens=num_outputs))\n",
    "\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "\n",
    "    service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "    index = VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "    index.storage_context.persist()\n",
    "\n",
    "    return index\n",
    "\n",
    "def ask_ai():\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"./storage\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context)\n",
    "    while True:\n",
    "        query_engine = index.as_query_engine()\n",
    "        query = input(\"Hello! How may I help today? \")\n",
    "        response = query_engine.query(query)\n",
    "        display(Markdown(f\"Response: <b>{response.response}</b>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vz1jp33jGumu"
   },
   "source": [
    "# Set OpenAI API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoJHE4fsAT3w"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVrddlAL4I_v"
   },
   "source": [
    "# Construct an index\n",
    "Now we are ready to construct the index. This will take every file in the folder 'data', split it into chunks, and embed it with OpenAI's embeddings API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCYTE2EqBB7O"
   },
   "outputs": [],
   "source": [
    "construct_index(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ipJ_gYxN5cWh"
   },
   "source": [
    "# Ask questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_uwsPGEIGsb"
   },
   "outputs": [],
   "source": [
    "ask_ai()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
